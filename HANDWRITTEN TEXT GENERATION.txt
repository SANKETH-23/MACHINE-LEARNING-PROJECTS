type this first
pip install tensorflow

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Step 1: Load and Preprocess the Dataset
# Load the dataset (adjust path to your CSV file)
df = pd.read_csv("handwritten.csv")

# Clean column names (remove any extra spaces)
df.columns = df.columns.str.strip()

# Print the column names to verify
print("Columns in the CSV file:", df.columns)

# Ensure the 'text' column exists or update the column name
if 'text' not in df.columns:
    print("Error: 'text' column not found. Available columns:", df.columns)
    # Optionally, use another column name that contains the handwritten-like text
    # text = ' '.join(df['your_column_name'].dropna())
else:
    # Join all text entries into one large string
    text = ' '.join(df['text'].dropna())

# Step 2: Preprocess the text
# Convert text to lowercase to normalize it
text = text.lower()

# Create a sorted list of unique characters
chars = sorted(set(text))

# Create mappings from characters to indices and vice versa
char_to_idx = {char: idx for idx, char in enumerate(chars)}  # Char to index
idx_to_char = {idx: char for idx, char in enumerate(chars)}  # Index to char

# Step 3: Prepare Input and Output Sequences
seq_length = 40  # Length of the input sequences
sequences = []
next_chars = []

# Generate sequences from the text
for i in range(0, len(text) - seq_length, seq_length):
    sequences.append([char_to_idx[char] for char in text[i:i + seq_length]])
    next_chars.append(char_to_idx[text[i + seq_length]])

# Convert sequences into numpy arrays
X = np.array(sequences)
y = np.array(next_chars)

# Reshape X to be of shape (num_samples, seq_length, 1)
X = X.reshape((X.shape[0], X.shape[1], 1))  # The 1 is because we're working with characters (not one-hot encoding)

# Step 4: Build the RNN Model
model = Sequential()

# First LSTM layer with return_sequences=True to allow stacking layers
model.add(LSTM(128, input_shape=(X.shape[1], 1), return_sequences=True))
model.add(Dropout(0.2))  # Dropout layer to prevent overfitting

# Second LSTM layer
model.add(LSTM(128))
model.add(Dropout(0.2))

# Output layer: Dense layer with softmax activation to predict next character
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Print the model summary
model.summary()

# Step 5: Train the Model
early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True)

# ModelCheckpoint callback to save the best weights during training
checkpoint = ModelCheckpoint('handwritten_text_rnn_weights.keras', save_best_only=True, monitor='loss', mode='min')

# Train the model
model.fit(X, y, batch_size=128, epochs=50, verbose=1, callbacks=[early_stopping, checkpoint])

# Step 6: Function to Generate New Text from the Model
def generate_text(model, seed_text, seq_length, char_to_idx, idx_to_char, temperature=1.0):
    generated_text = seed_text
    
    # If the seed_text is shorter than seq_length, pad it with spaces
    if len(seed_text) < seq_length:
        seed_text = seed_text + ' ' * (seq_length - len(seed_text))  # Padding with spaces
    
    for _ in range(500):  # Generate 500 characters
        # Prepare the input sequence for prediction
        input_sequence = [char_to_idx.get(char, char_to_idx[' ']) for char in generated_text[-seq_length:]]
        input_sequence = np.reshape(input_sequence, (1, seq_length, 1)) / len(char_to_idx)

        # Predict the next character probabilities
        predictions = model.predict(input_sequence, verbose=0)

        # Apply temperature for randomness in generation
        predictions = predictions
